---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am an Assistant Professor in the [College of Information and Computer Sciences (CICS)](https://www.cics.umass.edu/) at the University of Massachusetts Amherst, the flagship campus of the UMass system. I received my Ph.D. in Electrical Engineering from [North Carolina State University](https://www.ece.ncsu.edu/) in 2020. I am a member of the [Programming Language and Systems at Massachusetts (PLASMA)](https://plasma-umass.org/) lab at UMass. My research lies in machine learning systems, with an emphasis on improving the speed, scalability, and reliability of Machine Learning through innovations in algorithms and programming systems (e.g., compilers, runtime).  


Modern machine learning, especially deep learning, has made remarkable progress. However, its effective adoption faces a fundamental question: How can we create models that _efficiently_ deliver _reliable_ predictions to meet the requirements of  _diverse applications running on various systems_?
In response to this question, my group focuses on addressing the core challenges related to
(1) reducing the costs of machine learning model development and
(2) bringing deep learning applications to resource-constrained edge environments.
The approaches we pursue draw insights from the unique properties of machine learning workloads, 
such as their inherent accuracy-efficient trade-offs, as well as system design principles such as composability, pipelining, and locality awareness. 
Ultimately, our goal is to help democratize machine learning by transforming it into an accessible commodity technology that can be applied to a wide range of real-world scenarios. 


News
----
- **[Feb. 2024]**: Thanks for the support from the NSF for the CAREER award on [Adaptive Deep Learning Systems Towards Edge Intelligence](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2338512&HistoricalAwards=false). 


- **[Feb. 2024]**: Congratulations to Qizheng Yang for our work "GMorph: Accelerating Multi-DNN Inference via Model Fusion" accepted to [EurSys'24](https://2024.eurosys.org/).  

- **[Sept. 2023]**: Congratulations to [Kunjal Panchal](https://astuary.github.io/Kunjal/) for our work on "Flow: Per-instance Personalized Federated Learning" accepted to [NeurIPS'23](https://nips.cc/). 

- **[Sept. 2023]**: Congratulations to [Lijun Zhang](https://zhanglijun95.github.io/resume/) for the [2023 IBM PhD Fellowship Award](https://research.ibm.com/university/awards/fellowships.html). 

- **[Sept. 2023]**: Congratulations to [Sohaib Ahmad](https://sohaibahmad759.github.io/) for our work on "Proteus: A High-Throughput Inference-Serving System with Accuracy Scaling" accepted to [ASPLOS'24](https://www.asplos-conference.org/asplos2024/). 

- **[Sept. 2023]**: Thanks for the support of NSF to our project [Memory-Driven Full-Stack Collaboration for Embedded Systems](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2312396&HistoricalAwards=false). With collaborators, we will bring the power of deep learning to resource-constrained embedded systems! 

- **[Aug. 2023]**: Thanks for the support of NSF to our project [Deep Learning on Anomaly Detection for Human Dynamics and Hazard Response](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2220211&HistoricalAwards=false). With collaborators, we will work on graph machine learning for anomaly detection. 

- **[Aug. 2023]**: Congratulations to Juelin and Sandeep for their work on "Accelerating Subgraph Enumeration Using Auxiliary Graphs" accepted to [PACT'23](https://pact2023.github.io/). 

- **[May. 2023]**: Our work on "Flash: Concept Drift Adaptation in Federated Learning"  is accepted to [ICML'23](https://guanh01.github.io/files/2023flash.pdf). It proposes a novel adaptive optimizer that simultanuously addresses both data heterogeneity and the concept drift issues in federated learning.  

- **[May. 2023]**: Our work on "Automatically marginalized MCMC in probabilistic programming"  is accepted to [ICML'23](https://arxiv.org/pdf/2302.00564.pdf). It proposes automatic marginalization to make sampling process using Hamiltonian Monte Carlo more efficient.  

- **[May. 2023]**: Our work on "NUMAlloc: A Faster NUMA Memory Allocator" is accepted to ISMM'23. 

- **[May. 2023]**: Our work on "GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism" is on [Arxiv](https://arxiv.org/pdf/2303.13775.pdf). 

- **[Apr. 2023]**: Our work on [Re-thinking computation offload for efficient inference on IoT devices with duty-cycled radios](http://guanh01.github.io/files/2023mobicom.pdf) is accepted to [MobiCom'23](https://sigmobile.org/mobicom/2023/). 

- **[Oct. 2022]**: Iâ€™m excited to share that we have received an Amazon Research Award for our proposal "Groot: A GPU-Resident System for Efficient Graph Machine Learning" at UMass Amherst. Learn more about the program on the [website](https://amzn.to/ara-fall-winter-2021).  

- **[Sept. 2022]**: Our work on [AutoMTL: A Programming Framework for
Automating Efficient Multi-Task Learning](http://guanh01.github.io/files/2022automtl.pdf) is accepted to [NeurIPS'22](https://nips.cc/). Congratulations to [Lijun](https://zhanglijun95.github.io/resume/). The project is [open-sourced](https://github.com/zhanglijun95/AutoMTL)

- **[Sept. 2022]**: Thanks for the support of NSF to our project [Transparently Scaling Graph Neural Network Training to Large-Scale Models and Graphs](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2224054&HistoricalAwards=false).

- **[Jul. 2022]**: Our work on [Fine-Grained Personalized Federated Learning Through Dynamic Routing](http://guanh01.github.io/files/2022flow.pdf) is accepted to [CrossFL'2022 Workshop @MLSys](https://crossfl2022.github.io/program/). Congratulations to Kunjal. 

- **[Jul. 2022]**: Our work on [Improving Subgraph Representation Learning via Multi-View Augmentation](https://arxiv.org/pdf/2205.13038.pdf) is accepted to [AI4Science'22 Workshop @ICML](http://ai4science.net/icml22/schedule.html). 

- **[May. 2022]**: Our work "[A Tree-Structured Multi-Task Model Recommender](http://guanh01.github.io/files/2022automl.pdf)" is accepted to [AutoML'22](https://automl.cc/). Congratulations to [Lijun](https://zhanglijun95.github.io/resume/). The project is [open-sourced](https://github.com/zhanglijun95/TreeMTL). 

- **[May. 2022]**: Welcome a new PhD student Qizheng Yang to join our lab this summer.  

- **[Mar. 2022]**: Thanks for the support of [NVIDIA Academic Hardware Grant Program](https://mynvidia.force.com/HardwareGrant/s/Application) to the project "Multitasking-Centric Optimization for Deep Learning Applications".

- **[Mar. 2022]**: Our paper "[Rethinking Hard-Parameter Sharing in Multi-Domain Learning](http://guanh01.github.io/files/2022rethinking.pdf)" is accepted to [ICME'22](http://2022.ieeeicme.org/). Congratulations to [Lijun](https://zhanglijun95.github.io/resume/). 

- **[Mar. 2022]**: Our paper "[Enabling Near Real-Time NLU-Driven Natural Language Programming through Dynamic Grammar Graph-Based Translation](http://guanh01.github.io/files/2022cgo.pdf)" is accepted to [CGO'22](https://conf.researchr.org/home/cgo-2022).

- **[Mar. 2022]**: Our paper "[COMET: A Novel Memory-Efficient Deep Learning Training Framework by Using Error-Bounded Lossy Compression](https://arxiv.org/pdf/2111.09562.pdf)" is accepted to [VLDB'22](https://vldb.org/2022/).

- **[Nov. 2021]**: Our collaborative project with [Prof. Zhou Lin](https://www.chem.umass.edu/faculty/zhou-lin) on "Accelerating Fragment-Based Quantum Chemistry via Machine Learning" received [UMass ADVANCE Collaborative Research Seed Grant](https://www.umass.edu/advance/find-funding/collaborative-research-seed-grants/collaborative-research-seed-grant-recipients-fall-1). 

- **[Oct. 2021]**: Our paper "[FreeLunch: Compression-based GPU Memory Management for Convolutional Neural Networks](http://guanh01.github.io/files/2021mchpc.pdf)" is accepted to [MCHPC'21 Workshop](https://passlab.github.io/mchpc/mchpc2021/), in conjunction with SC'21.

- **[Oct. 2021]**: Our paper "[Recurrent Neural Networks Meet Context-Free Grammar: Two Birds with One Stone](http://guanh01.github.io/files/2021rnn.pdf)" is accepted to [ICDM'21](https://icdm2021.auckland.ac.nz/).

- **[June 2021]**: Our paper "[Scalable Graph Neural Network Training: The Case for Sampling](http://guanh01.github.io/files/2021sampling.pdf)" has appeared in the ACM SIGOPS Operating Systems Review.

- **[June 2021]**: Our paper [CoCoPIE](https://cacm.acm.org/magazines/2021/6/252819-cocopie/fulltext) is accepted to CACM'21. 

- **[June 2021]**: Our paper [NumaPerf](http://guanh01.github.io/files/2021ics.pdf) is accepted to ICS'21.  

- **[May 2021]**: I have received an [Adobe Research Collaboration Grant](https://research.adobe.com/collaborations/) on developing resource-efficient deep multi-task learning solutions.

- **[May 2021]**: Our paper "[Reuse-Centric Kmeans Configuration](https://www.sciencedirect.com/science/article/abs/pii/S0306437921000430)" is accepted to Information Systems. Congratulations to [Lijun](https://zhanglijun95.github.io/resume/). 


Awards
---- 
- NSF CAREER Award, 2024 
- NCSU Electrical and Computer Engineering Outstanding Dissertation Award, 2020 
- IBM PhD Fellowship, 2015-2018







